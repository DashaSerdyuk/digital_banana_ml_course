{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Natural Langauge Processing (обработка естественного языка)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP - большая, интересная и важная область ИИ. Ей уже много лет, но несмотря на ее возраст, не на все задачи есть решения.\n",
    "Сегодня у нас обзорная лекция по задачам NLP с некоторыми примерами и увлекательной домашкой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные задачи (классификация взята из лекции Д.Бугайченко, https://ok.ru/live/755772235697)\n",
    " - на уровне сигналов: распознавание речи (speech recognition), синтез речи\n",
    " - на уровне слов: коррекция ошибок (error correction), нормализация слов (лемматизация, стемминг), морфологический анализ (morph analysis/segmentation)\n",
    " - на уровне коллокаций (словосочетаний): частеречная разметка (POS-tagging), извлечение именованных сущностей (Named Entity Recignition), сегментация слов (word segmentation)\n",
    " - на уровне предложений: сегментация предложений (sentence segmentation), query answering, parsing, разрешение лексической неоднозначности (word sense disambiguation)\n",
    " - на уровне абзацев: разрешение кореференции (coreference resolution), определение языка (language detection), определение тональности текста (sentiment analysis)\n",
    " - на уровне документов: автоматическое реферирование (text summarization), машинный перевод (machine translation), тематическое моделирование (topic modeling)\n",
    " - на уровне корпуса: определение дубликатов (deduplication), извлечение информации (informational retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentiment analysis \n",
    "(from https://www.cloudbeds.com/articles/perform-sentiment-analysis-reviews/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sentiment-analysis](https://wkusaapbz93eciyl3ze4vh1a-wpengine.netdna-ssl.com/wp-content/uploads/2016/08/Sentiment-Analysis-Review-Highlights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS-tagging\n",
    "(from https://rewordify.com/helppos.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://rewordify.com/images/posselect.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделы языкознания:\n",
    "    - фонетика\n",
    "    - морфология\n",
    "    - лексикология\n",
    "    - синтаксис\n",
    "    - семантика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Фонетика__\n",
    "\n",
    "Перевести сигнал в фонему - нетривиальная задача. К примеру, flick и lick звучат почти одинаково, mean/green - тоже схоже по звучанию. Раньше на коне были скрытые марковские модели, сейчас рулят нейронки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Морфология__\n",
    "\n",
    "\"корова\" - существительное женского рода, корень \"коров\", окончание \"а\", единственное число, именительный падеж. Под морфологическим анализом понимается определение леммы (базовая форма слова) и его грамматических характеристик. \n",
    "\n",
    "Здесь сталкиваемся с задачей нормализации - приведению слова к канонической форме. К примеру, слово \"собаку\"  нам надо привести к \"собака\", или \"красивый\" в \"красив\". Это можно сделать двумя разными способами (не путать их между собой!):\n",
    " - лемматизация\n",
    " - стемминг\n",
    " \n",
    "_Стемминг_ - это процесс нахождения основы слова для заданного исходного слова. Основа слова не обязательно совпадает с морфологическим корнем слова (википедия). Реализации -  Mystem, Porter, Showball и еще и еще\n",
    "\n",
    "_Лемматизация_ - процесс приведения словоформы к лемме — её нормальной (словарной) форме. Здесь уже, обычно, без словарей не обойтись точно. В стемминге можно было :) Реализации: самый популярный - pymorphy2.\n",
    "\n",
    "\n",
    "Примеры в студию, сектор приз на барабане!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходник        Лемматизация    Стемминг       \n",
      "\n",
      "был             быть            был            \n",
      "коровы          корова          коров          \n",
      "красавчиком     красавчик       красавчик      \n",
      "огурцы          огурец          огурц          \n",
      "летал           летать          лета           \n",
      "Будапеште       будапешт        будапешт       \n",
      "гуляшом         гуляш           гуляш          \n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "stemmer = RussianStemmer()\n",
    "\n",
    "example_words = \"собраться можно и потом же. не?\"\n",
    "print(\"{0:15} {1:15} {2:15}\\n\".format(\"Исходник\", \"Лемматизация\", \"Стемминг\"))\n",
    "for word in example_words.split(\" \"):\n",
    "    print(\"{0:15} {1:15} {2:15}\".format(word, morph.parse(word)[0].normal_form, stemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "С __лексикой__ мы сталкиваемся в задаче фильтрации обсценной лексики. Или в детектировании слэнга и диалектизмов. Также существует задача разрешения лексической многозначности. К примеру, в предложениях \"Python - распространенный язык программирования\" и \"Из-за анестезии у него онемел язык\" слово \"язык\" имеет разные значения.\n",
    "\n",
    "При решении многих задач NLP необходима информация о структуре предложения. Для этого на помощь приходит __синтаксис__ с его парсингом (в смысле синтаксического анализа). Основные подходы заключаются на грамматиках и теории синтаксических групп."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Семантика__ отвечат за смысловое значение единиц языка. Сейчас вся наука стремится к созданию моделей с сильными семантическими свойствами, чтобы машина реально научилась понимать смысл. Отдельно хочу отметить задачу детекции сарказма - очень нетривиальная и интересная задача."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Языкознанием языкознанием, но как заставить компьютер воспринимать текст? \n",
    "- можно посимвольно, почему бы и нет, кстати, это дает определенный профит, но об это попозже\n",
    "- логика говорит нам, что хорошо бы по словам\n",
    "\n",
    "Встает вопрос - как работать с текстом? Надо как-то представить текст в виде векторов. Есть два наиболее популярных подхода:\n",
    " - метод bag of words\n",
    " - эмбеддинги на основе нейросетевых архитектур (word2vec, GLOVE)\n",
    " \n",
    " fasttext -  посмотреть, что там под капотом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Суть \"мешка слов\" легка и понятна. Слова рассматриваются вне зависимости от их порядка в предложении. Также в этом подходе существует понятие n-граммы - словосочетания, состоящего из n слов. \n",
    "\n",
    "Есть предложение \"Дети играли на лесной опушке\". Тогда, для него набором биграмм будет таким: \"дети играли\", \"играли на\", \"на лесной\", \"лесной опушке\". Если триграмм, то \"дети играли на\", \"играли на лесной\", \"на лесной опушке\". Униграммы - это просто, по сути, токены. В данном случае униграммы в предложении будут такими: \"дети\", \"играли\", \"на\", \"лесной\", \"опушке\".\n",
    "\n",
    "Почему n-граммы так важны? К примеру, они позволяют частично разрешать лексическую многозначность. К примеру, у нас есть еще одно предложение - \"На станции Лесной был найден бесхозный пакет\".\n",
    "Итак, мы имеем два совершенно разных по смыслу предложения. Но с одним общим словом - \"лесной\".\n",
    "\n",
    "Совпадают эти предложения по двум словам - \"на\" и \"лесной\". Но если мы посмотрим на биграммы, то благодаря соседним словам \"опушка\" и \"станция\" мы автоматически снимаем лексическую мноозначность - т.е. мы точно понимаем, что в первом предложении \"лесной\" - это про лес, а во втором что-то с метро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все равно вопрос не уходит - как слова перегнать в вектора? В работе с bag of words строят матрицы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все токены: {'перед', 'опушка', 'метро', 'станция', 'рекой', 'лесная'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#               sent_1                          sent_2\n",
    "corpus = [\"Лесная опушка перед рекой\", \"Станция метро Лесная\"]\n",
    "tokens = [x.lower().split(\" \") for x in corpus]\n",
    "\n",
    "tokens = np.concatenate(tokens)\n",
    "print(\"Все токены: {0}\".format(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем матрицу, по строкам будут наши предложения, по столбцам - все уникальные слова. Таким образом, у нас будет матрица вида:\n",
    "\n",
    "$\\begin{pmatrix} \n",
    "\"-\" & перед & опушка & метро & станция & рекой & лесная \\\\\n",
    "sent_1 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n",
    "sent_2 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем заполнять? Можно бинарно: если слово есть - 1, нет - 0. Тогда получим:\n",
    "\n",
    "$\\begin{pmatrix} \n",
    "- & лесная & метро & опушка & перед & рекой & станция \\\\\n",
    "sent_1 & 1 & 0 & 1 & 1 & 1 & 0\\\\ \n",
    "sent_2 & 1 & 1 & 0 & 0 & 0 & 1\n",
    "\\end{pmatrix} $\n",
    "\n",
    "Далее: пример кода, который строит такую матрицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['лесная', 'метро', 'опушка', 'перед', 'рекой', 'станция']\n",
      "[[1 0 1 1 1 0]\n",
      " [1 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "count_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(count_vectorizer.get_feature_names())\n",
    "print(count_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть более хитрая метрика - TF-IDF (term frequency - inverse document frequency).  Подробнее: здесь - https://ru.wikipedia.org/wiki/TF-IDF. Ее смысл во взвешивании слов, она дает бОльший вес редким словам, а у общеупотребительных \"занижает\" вес, образно выражаясь. \n",
    "\n",
    "Код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['лесная', 'метро', 'опушка', 'перед', 'рекой', 'станция']\n",
      "[[0.37997836 0.         0.53404633 0.53404633 0.53404633 0.        ]\n",
      " [0.44943642 0.6316672  0.         0.         0.         0.6316672 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer =  TfidfVectorizer(ngram_range=(1,1))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names())\n",
    "print(tfidf_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь уже не так легко руками посчитать, но возможно :) Эта метрика очень популярна и не зря. Она понятна и часто используется в боевых условиях.\n",
    "\n",
    "Bag of words - не единственный подход к представлению текста в вектора. Есть еще word2vec (и Glove, и другие). Пример работы с word2vec описан в соседнем одноименном ноутбуке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ок, мы умеем из текста делать вектора. Что дальше?\n",
    "\n",
    "А теперь уже можно строить модели, генерировать фичи, решать задачи наконец :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итак, важные шаги при работе с текстом:\n",
    "    \n",
    "    1) поиск корпуса\n",
    "    2) нормализация слов (лемматизация/стемминг)\n",
    "    3) удаление стоп-слов, знаков препинания, приведение слов к нижнему регистру\n",
    "    4) также иногда требуется замена токенов на специальный тег. к примеру, все цифры в корпусе заменить на тег <NUMBER>, к примеру. Все зависит от задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А дальше переходим к практике!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
