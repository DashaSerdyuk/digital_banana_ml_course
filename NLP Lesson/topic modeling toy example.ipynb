{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"michael_jackson_lyrics_corpus.csv\", encoding='utf-8')\n",
    "df.columns = ['text']\n",
    "data = list(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "matrix = vectorizer.fit_transform(data)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "random.shuffle(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "passes = 40\n",
    "tm_model = LdaModel(matutils.Sparse2Corpus(matrix), \n",
    "                   num_topics=num_topics, passes=passes,id2word=dict([(i, s) for i, s in enumerate(vocabulary)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.153*\"mountain\" + 0.131*\"woo\" + 0.078*\"excuses\" + 0.078*\"young\" + 0.059*\"homeless\" + 0.044*\"called\" + 0.044*\"erase\" + 0.042*\"begins\" + 0.021*\"standing\" + 0.016*\"bathe\"'),\n",
       " (1,\n",
       "  '0.047*\"written\" + 0.034*\"understands\" + 0.034*\"lived\" + 0.029*\"abuse\" + 0.027*\"their\" + 0.026*\"begins\" + 0.026*\"hold\" + 0.024*\"cursed\" + 0.024*\"he\" + 0.024*\"castle\"'),\n",
       " (2,\n",
       "  '0.119*\"taking\" + 0.077*\"each\" + 0.075*\"makes\" + 0.053*\"closet\" + 0.049*\"gave\" + 0.039*\"disappear\" + 0.037*\"anything\" + 0.029*\"that\" + 0.029*\"funky\" + 0.024*\"nigh\"'),\n",
       " (3,\n",
       "  '0.094*\"voice\" + 0.071*\"him\" + 0.061*\"diaaaaaana\" + 0.060*\"as\" + 0.045*\"final\" + 0.045*\"left\" + 0.041*\"more\" + 0.034*\"answered\" + 0.032*\"within\" + 0.031*\"excuses\"'),\n",
       " (4,\n",
       "  '0.155*\"childhood\" + 0.145*\"kissing\" + 0.065*\"hurting\" + 0.063*\"madness\" + 0.061*\"promise\" + 0.054*\"stop\" + 0.042*\"everyday\" + 0.040*\"counting\" + 0.034*\"youre\" + 0.034*\"waiting\"')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–≤—ã–≥–ª—è–¥–∏—Ç —Å—É–ø–µ—Ä, –Ω–æ –µ—Å—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –∞ –∏—Ö –±—ã —É–±—Ä–∞—Ç—å...ü§î –∏ –≤–æ–æ–±—â–µ –ø—Ä–∏—á–µ—Å–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, –º—ã –∑–∞–ø—É—Å—Ç–∏–ª–∏ \"–∫–∞–∫ –µ—Å—Ç—å\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopwords = set(stopwords.words(\"english\")) | set([\"im\",\"dont\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "for song in data:\n",
    "    tokens = list(set(song.split(\" \")))\n",
    "    words = [x for x in tokens if x not in english_stopwords]\n",
    "    clean_song = \" \".join(words) \n",
    "    clean_data.append(clean_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_clean = CountVectorizer(ngram_range=(1,1))\n",
    "matrix_clean =  vectorizer_clean.fit_transform(clean_data)\n",
    "vocabulary_clean = vectorizer_clean.get_feature_names()\n",
    "random.shuffle(vocabulary_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "passes = 40\n",
    "tm_model_clean = LdaModel(matutils.Sparse2Corpus(matrix_clean), \n",
    "                    num_topics=num_topics, \n",
    "                    passes=passes,\n",
    "                    id2word=dict([(i, s) for i, s in enumerate(vocabulary_clean)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"lam\" + 0.023*\"look\" + 0.021*\"tired\" + 0.020*\"knows\" + 0.019*\"speaks\" + 0.019*\"grandmas\" + 0.019*\"dreams\" + 0.018*\"would\" + 0.018*\"runs\" + 0.018*\"happiness\"'),\n",
       " (1,\n",
       "  '0.090*\"school\" + 0.082*\"sandy\" + 0.072*\"songbird\" + 0.072*\"serenading\" + 0.062*\"enough\" + 0.044*\"shines\" + 0.035*\"carousel\" + 0.031*\"yearning\" + 0.028*\"wouldnt\" + 0.025*\"looking\"'),\n",
       " (2,\n",
       "  '0.073*\"ahwalkin\" + 0.065*\"tired\" + 0.062*\"certain\" + 0.055*\"role\" + 0.052*\"big\" + 0.045*\"sail\" + 0.031*\"sense\" + 0.027*\"castle\" + 0.026*\"problems\" + 0.025*\"would\"'),\n",
       " (3,\n",
       "  '0.111*\"happen\" + 0.111*\"house\" + 0.079*\"education\" + 0.059*\"talkin\" + 0.052*\"ravaging\" + 0.038*\"see\" + 0.018*\"monkey\" + 0.017*\"vain\" + 0.016*\"doomed\" + 0.015*\"young\"'),\n",
       " (4,\n",
       "  '0.079*\"coz\" + 0.071*\"gods\" + 0.064*\"cramp\" + 0.051*\"vain\" + 0.045*\"help\" + 0.036*\"ones\" + 0.034*\"shores\" + 0.031*\"cast\" + 0.028*\"preacher\" + 0.026*\"following\"')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_model_clean.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " –∞ —Ç–µ–ø–µ—Ä—å –µ—â–µ –ø—Ä–æ–π–¥–µ–º—Å—è —Å—Ç–µ–º–º–∏–Ω–≥–æ–º!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_data = []\n",
    "for song in clean_data:\n",
    "    tokens = list(set(song.split(\" \")))\n",
    "    words = [stemmer.stem(t) for t in tokens]\n",
    "    stem_song = \" \".join(words) \n",
    "    stem_data.append(stem_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_stem = CountVectorizer(ngram_range=(1,1))\n",
    "matrix_stem =  vectorizer_stem.fit_transform(clean_data)\n",
    "vocabulary_stem = vectorizer_stem.get_feature_names()\n",
    "random.shuffle(vocabulary_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "passes = 40\n",
    "tm_model_stem = LdaModel(matutils.Sparse2Corpus(matrix_stem), \n",
    "                    num_topics=num_topics, \n",
    "                    passes=passes,\n",
    "                    id2word=dict([(i, s) for i, s in enumerate(vocabulary_stem)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.073*\"sometime\" + 0.072*\"boyfriend\" + 0.065*\"mojo\" + 0.056*\"meeting\" + 0.053*\"wrap\" + 0.049*\"waking\" + 0.040*\"thick\" + 0.037*\"blue\" + 0.035*\"gang\" + 0.030*\"holy\"'),\n",
       " (1,\n",
       "  '0.152*\"flee\" + 0.047*\"unfold\" + 0.046*\"painful\" + 0.038*\"heartaches\" + 0.037*\"raise\" + 0.025*\"macho\" + 0.021*\"stare\" + 0.019*\"open\" + 0.010*\"counting\" + 0.003*\"choice\"'),\n",
       " (2,\n",
       "  '0.068*\"earth\" + 0.063*\"ear\" + 0.053*\"fifteen\" + 0.051*\"product\" + 0.041*\"wifes\" + 0.040*\"gettin\" + 0.037*\"burnt\" + 0.036*\"thighs\" + 0.036*\"sang\" + 0.030*\"daylight\"'),\n",
       " (3,\n",
       "  '0.119*\"see\" + 0.084*\"standing\" + 0.081*\"we\" + 0.073*\"felt\" + 0.040*\"name\" + 0.037*\"perfect\" + 0.028*\"women\" + 0.026*\"beginning\" + 0.023*\"yes\" + 0.019*\"abandoned\"'),\n",
       " (4,\n",
       "  '0.039*\"motormouth\" + 0.031*\"counting\" + 0.028*\"whatever\" + 0.028*\"little\" + 0.028*\"shit\" + 0.027*\"helpless\" + 0.027*\"tonight\" + 0.024*\"candy\" + 0.022*\"lord\" + 0.022*\"choice\"')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_model_stem.show_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
